# Set a prior for the shared treatment effect of BPD
bpd_effect_prior <- prior(normal(0, 10), class = "b", coef = "bpd")
# Set a prior for the variability in BPD effect across studies
bpd_study_sd_prior <- prior(cauchy(0, 0.5), class = "sd", group = "study")
# Combine all the priors
ma_priors <- c(study_priors, bpd_effect_prior, bpd_study_sd_prior)
# Chunk 22
tryCatch({
# Your R code that might cause errors
}, error = function(e) {
# Write the error to a log file
writeLines(as.character(e$message), "error_log.txt")
})
ma_formula <- bf(events_n |
trials(total_n) ~ 0 + #Remove the intercept from the model (such that the risk of stroke is not modeled using a common term but is modeled separately for each study)
factor(study) + #A term to refer to each study
bpd + # Fixed treatment effect of bpd
(bpd - 1 |study)) # We allow for a random slope, but a fixed intercept effect
options(mc.cores = parallel::detectCores())
ma_model <-
brm(
ma_formula,
data = combined_data,
family = binomial(),
prior = ma_priors,
seed = 100,
control = list(adapt_delta = 0.99),verbose=FALSE
)
# Chunk 23: plotting
#ma_model=readRDS("~/Library/CloudStorage/Dropbox-Personal/ma_model.rds")
sum_ma_model <- summary(ma_model)
print(sum_ma_model)
#Get the parameters of the row which corresponds to the treatment (BPD)
ma_bpd_par <- sum_ma_model$fixed[rownames(sum_ma_model$fixed) == "bpd", ]
##Store relevant variables
#The log odds ratio
ma_bpd_lnor <- round(ma_bpd_par$Estimate, 2)
#Standard error of the log odds ratio
ma_bpd_lnor_sd <- round(ma_bpd_par$Est.Error, 2)
#The odds ratio
ma_bpd_or <- round(exp(ma_bpd_par$Estimate), 2)
#The lower limit of the 95% credible interval of the odds ratio
ma_bpd_or_lci <- round(exp(ma_bpd_par$`l-95% CI`), 2)
#The upper limit of the 95% credible interval of the odds ratio
ma_bpd_or_uci <- round(exp(ma_bpd_par$`u-95% CI`), 2)
# Chunk 24
#Sample the posterior distribution of study-level estimates and the overall estimate of treatment effect
study_es <- ma_model %>%
spread_draws(r_study[study, ], b_bpd) %>%
mutate(b_bpd = r_study + b_bpd, #Create treatment effect estimates for each study
type = "Study-level estimate") #Clarify that this is the treatment effect for each study
pooled_es <- spread_draws(ma_model, b_bpd) %>%
mutate(study = "Overall Effect Size", #Clarify that this is the pooled/overall treatment effect
type = "Pooled estimate") #Same
#Exponentiate to get odds instead of log-odds ratio
fp_data <- bind_rows(study_es, pooled_es) %>%
mutate(b_bpd = b_bpd %>% exp)
# Assuming 'author' is the column in your dataset that contains these names
# Replace 'Goldber' with 'Clear Wisdom'
fp_data$study <- gsub("goldberg", "CLEAR Wisdom", fp_data$study)
# Replace 'Laufs' with 'Clear Serenity'
fp_data$study <- gsub("laufs", "CLEAR Serenity", fp_data$study)
# Replace 'Ray' with 'Clear Harmony'
fp_data$study <- gsub("ray", "CLEAR Harmony", fp_data$study)
#Create title and subtitles for the plot
main_title <- "Meta-analysis-based prior for the effectiveness of BPD in preventing MACE"
subtitle <- "This analysis synthesizes the 3 RCTs constituting the RCT evidence base for BPD prior to the CLEAR Trial."
library(forcats)
# Assuming fp_data$study is a factor with levels ordered as you originally had them
# Reverse the levels to put 'Overall Effect Size' at the bottom
fp_data$study <- fct_rev(fp_data$study)
# Now plot your data with ggplot2
g=ggplot(data = fp_data, aes(y = study, x = b_bpd, fill = type)) +
geom_density_ridges(col = NA, scale = 0.9, alpha = 0.7) +
geom_vline(xintercept = 1, color = "black", lwd = 1, linetype = 2) +
scale_fill_manual(values = c("salmon","lightblue")) + # Make sure the colors are in the right order
ggtitle(main_title, subtitle = subtitle) +
scale_y_discrete(name = "Study") +
scale_x_continuous(name = "Odds Ratio", trans = "log", breaks = c(0.5, 1, 2)) +
coord_cartesian(xlim = c(0.5, 2)) +
theme_pubclean() +
theme(text = element_text(size = 23),
plot.title = element_text(face = "bold", hjust = 0.0, size = 20),
plot.subtitle = element_text(face = "bold", size = 15, hjust = 0.0, color = "grey45"),
axis.text.x = element_text(size = 20, face = "bold"),
axis.text.y = element_text(size = 15, face = "bold"),
axis.title.x = element_text(size = 25, face = "bold"),
axis.title.y = element_blank(),
axis.line = element_line(colour = "black", linewidth = 1.2),
plot.margin = margin(0.5, 1, 0.5, 1, "cm"),
legend.background = element_rect(fill = "transparent"),
legend.position = "none",
legend.text = element_text(size = 12, face = "bold"),
legend.key.width = unit(1.5, "cm"),
legend.key.height = unit(0.75, "cm"))
g
ggsave(g,file="~/Library/CloudStorage/Dropbox-Personal//metaplot.png",width = 15,height=10)
# Chunk 25
tryCatch({
# Your R code that might cause errors
}, error = function(e) {
# Write the error to a log file
writeLines(as.character(e$message), "error_log.txt")
})
# Enter the data from the table into a data frame
meta_data <- data.frame(
study = c("Goldberg et al., 2019", "Laufs et al., 2019", "Ray et al., 2019", "Nissen et al., 2023"),
group = rep(c("Bempedoic Acid", "Placebo"), each = 4),
events = c(32, 9, 68, 831, 21, 1, 42, 927), ## add a 1 to make it computable
total = c(522, 234, 1487, 6992, 257, 111, 742, 6978)
)
# Likelihood data from the Nissen study
likelihood_data <- meta_data[meta_data$study == "Nissen et al., 2023",]
ma_bpd_lnor <- round(ma_bpd_par$Estimate, 2)
#Standard error of the log odds ratio
ma_bpd_lnor_sd <- round(ma_bpd_par$Est.Error, 2)
# Convert 'group' to a factor and create a binary treatment indicator
likelihood_data$bpd <- ifelse(likelihood_data$group == "Bempedoic Acid", 1, 0)
library(brms)
# Define the prior
# Define the mean and standard deviation for the log OR
log_or_mean <- ma_bpd_lnor  # Mean log OR
log_or_sd <- ma_bpd_lnor_sd    # Standard deviation of the log OR
# Assuming 'bpd' is the name of the variable for the treatment effect in your model
# Set the prior using the mean and standard deviation
nissen_prior <- set_prior(
paste("normal(", log_or_mean, ",", log_or_sd, ")"),
class = "b",
coef = "bpd"
)
# Verify the prior
nissen_model <- brm(
formula = bf(events | trials(total) ~ bpd),  # Update 'bpd' if your variable name is different
data = likelihood_data,
family = binomial(),
prior = nissen_prior,
seed = 102,
control = list(adapt_delta = 0.95),verbose=FALSE
)
#saveRDS(nissen_model,"~/Library/CloudStorage/Dropbox-Personal/nissen_model.rds")
# Chunk 26
#nissen_model=readRDS("~/Library/CloudStorage/Dropbox-Personal/nissen_model.rds")
summary(nissen_model)
# Chunk 27
# Given data from the table
n_treatment <- 6992
events_treatment <- 831
n_control <- 6978
events_control <- 927
# Calculate the odds ratio (OR)
or <- (events_treatment / (n_treatment - events_treatment)) / (events_control / (n_control - events_control))
# Convert OR to logOR
log_or <- log(or)
# Calculate the standard error of the logOR
se_log_or <- sqrt((1 / events_treatment) + (1 / (n_treatment - events_treatment)) + (1 / events_control) + (1 / (n_control - events_control)))
calc_conjugate_posterior(log_or = log_or,se = se_log_or,prior = list(mu=ma_bpd_lnor,sigma=ma_bpd_lnor_sd))
pm=calc_conjugate_posterior(log_or = log_or,se = se_log_or,prior = list(mu=ma_bpd_lnor,sigma=ma_bpd_lnor_sd))
## we shos that these are quite close and differ likely because we also place a prior on baseline effects here
# Chunk 28
prior_sim <- rnorm(n = 10000, #Number of simulations
mean = ma_bpd_lnor, #The log-OR from our meta-analysis
sd = ma_bpd_lnor_sd) #The standard error of the log-OR from our meta-analysis
#likelihood sim
likelihood_sim <- rnorm(n = 10000,
mean = log_or,
sd = se_log_or)
## posterior sim
post_sim <- rnorm(n = 10000,
mean = -0.13,
sd = 0.05)
##Let us recap the 3 distributions we now have:
# "prior_sim", which contains our prior (obtained from the meta-analysis)
# "likelihood_sim" which contains the data from the CLEAR2 Ttrial
# "post_sim" which contains the data from our posterior (the combination of the above 2)
# Chunk 29
#First, let us create a dataframe containing the above 3 simulations
sims <- data.frame(sim_lnor = c(prior_sim, likelihood_sim, post_sim), #This column contains the results of our simulations (each of length 10,000)
sim_type = rep(c("Prior", "Likelihood", "Posterior"), each = 10000) #This column contains the labels of these simulations so we can identify which simulation each row belongs to
)
#Arrange sim_type so that it shows up with the prior on top, the posterior at the bottom, and likelihood in the middle.
sims$sim_type <- factor(sims$sim_type, levels = c("Posterior", "Likelihood", "Prior"))
#Create title and subtitles for the plot
main_title <- "Posterior estimate: Bempedoic Acid Effect"
subtitle <- "Prior evidence is based on a meta-analysis of 3 RCTs testing Bempedoic Acid"
#Now, let us visualize this:
#Plot
g1=ggplot(data = sims,
aes(y = sim_type,
x = exp(sim_lnor),
fill = sim_type
)) +
#Add Density plots
stat_halfeye(alpha = 0.7, .width = 0.95) +
#Set colors
scale_fill_manual(name = "Information source:",
values = c("lightblue", "darkolivegreen", "salmon")) +
#Create title
ggtitle(main_title,
subtitle = subtitle) +
geom_vline(xintercept = 1, color = "black",
lwd = 1, linetype = 2) +
#Set x-axis limit
coord_cartesian(xlim = c(0.5, 2.0)) +
#X and Y axes aesthetics
scale_y_discrete(name = NULL, expand = c(0, 0.03)) +
scale_x_continuous(name = "Odds Ratio",
trans = "log",
breaks = c(0.5, 1, 2)) +
#Set theme
theme_pubclean() +
theme(text = element_text(size = 23),
plot.title=element_text(face = "bold", hjust = 0.0, size = 18),
plot.subtitle = element_text(face = "bold", size = 10, hjust = 0.0, color = "grey45"),
axis.text.x = element_text(size = 15, face = "bold"),
axis.text.y = element_text(size = 15, face = "bold", hjust = 0.5),
axis.title.x = element_text(size = 20, face = "bold"),
axis.title.y = element_blank(),
axis.line = element_line(colour = "black", linewidth = 1.2),
plot.margin = margin(0.5, 1, 0.5, 1, "cm"),
legend.background = element_rect(fill = "transparent"),
legend.position = "bottom",
legend.text = element_text(size = 16, face = "bold"),
legend.key.width = unit(1.5, "cm"),
legend.key.height = unit(0.75, "cm")
)
g1
ggsave(g1,file="~/Library/CloudStorage/Dropbox-Personal/post.png",width = 11,height=10)
# Chunk 30
pnorm(0,mean= -0.13 ,sd = 0.05)
pnorm(log_mcid,mean= -0.13 ,sd = 0.05)
summary(nissen_model)
list(
log_or = log_or,
se_log_or = se_log_or,
skeptical_prior = skeptical_prior,
enthusiastic_prior = enthusiastic_prior,
pessimistic_prior = pessimistic_prior
)
# Install necessary packages if not already installed
if (!require("metafor")) install.packages("metafor")
library(rstan)
library(metafor)
library(reshape)
library(ggplot2)
library(MASS)
library(ggplot2)
library(reshape2)  # For melting data frames
library(gridExtra)  #
library(ggpubr)
# Load necessary library
library(tidyverse)
# Given data from the table
n_treatment <- 6992
events_treatment <- 819
n_control <- 6978
events_control <- 927
# Calculate the odds ratio (OR)
or <- (events_treatment / (n_treatment - events_treatment)) / (events_control / (n_control - events_control))
# Convert OR to logOR
log_or <- log(or)
# Calculate the standard error of the logOR
se_log_or <- sqrt((1 / events_treatment) + (1 / (n_treatment - events_treatment)) + (1 / events_control) + (1 / (n_control - events_control)))
# Define the MCID as an absolute risk reduction of 1.3%
mcid_arr <- 0.013
# Convert the absolute risk reduction to logOR for MCID
# Assuming a baseline risk from the control group
baseline_risk <- events_control / n_control
# Calculate the risk in the treatment group
risk_treatment <- baseline_risk - mcid_arr
# Calculate the odds for control and treatment
odds_control <- baseline_risk / (1 - baseline_risk)
odds_treatment <- risk_treatment / (1 - risk_treatment)
# Calculate the odds ratio (OR)
or <- odds_treatment / odds_control
# For Bayesian analysis, we often use the natural log of the OR
log_or <- log(or)
# Define the priors based on the specifications
# Skeptical Prior: 90% certainty of no clinically relevant effect, so only 10% of area
skeptical_sd <- abs(log_mcid / qnorm(0.10))
skeptical_prior <- list(mu = 0, sigma = skeptical_sd)
# Enthusiastic Prior: 30% probability of harm
enthusiastic_sd <- abs(log_mcid) / qnorm(0.70)
enthusiastic_prior <- list(mu = log_mcid, sigma = enthusiastic_sd)
# Pessimistic Prior: 30% probability of benefit
pessimistic_sd <- abs(log_mcid) / qnorm(0.70)
pessimistic_prior <- list(mu = -log_mcid, sigma = pessimistic_sd)
# Output the calculated values and the priors
list(
log_or = log_or,
se_log_or = se_log_or,
skeptical_prior = skeptical_prior,
enthusiastic_prior = enthusiastic_prior,
pessimistic_prior = pessimistic_prior
)
list
ls()
print(x)
x="hello"
print(z)
print(x)
detach("package:StanHeaders", unload = TRUE)
detach("package:ggsci", unload = TRUE)
ls()
s=
results='hide',
# Given data from the table
n_treatment <- 6992
events_treatment <- 819
n_control <- 6978
events_control <- 927
# Calculate the odds ratio (OR)
or <- (events_treatment / (n_treatment - events_treatment)) / (events_control / (n_control - events_control))
# Convert OR to logOR
log_or <- log(or)
# Calculate the standard error of the logOR
se_log_or <- sqrt((1 / events_treatment) + (1 / (n_treatment - events_treatment)) + (1 / events_control) + (1 / (n_control - events_control)))
mcid_arr <- 0.028
# Assuming a baseline risk from the control group
(baseline_risk <- events_control / n_control)
risk_treatment <- baseline_risk - mcid_arr
# Calculate the odds for control and treatment
odds_control <- baseline_risk / (1 - baseline_risk)
odds_treatment <- risk_treatment / (1 - risk_treatment)
# Calculate the odds ratio (OR)
or_mcid <- odds_treatment / odds_control
# For Bayesian analysis, we often use the natural log of the OR
log_mcid <- log(or_mcid)
# here there is a 10% probability of the log_mcid
z_skeptical <- qnorm(0.10)
sigma_skeptical <- abs(log_mcid / z_skeptical)
pnorm(log_mcid,sd=sigma_skeptical)
z_enthusiastic <- qnorm(0.70)
sigma_enthusiastic <- abs((0 - log_mcid) / z_enthusiastic)
pnorm(0,mean=log_mcid,sd = sigma_enthusiastic)
z_pessimistic <- qnorm(0.30)
sigma_pessimistic <- abs((0 + log_mcid) / z_pessimistic)
pnorm(0,mean=-log_mcid,sd = sigma_pessimistic)
# Load necessary library
library(tidyverse)
# Given data from the table
n_treatment <- 6992
events_treatment <- 819
n_control <- 6978
events_control <- 927
# Calculate the odds ratio (OR)
or <- (events_treatment / (n_treatment - events_treatment)) / (events_control / (n_control - events_control))
# Convert OR to logOR
log_or <- log(or)
# Calculate the standard error of the logOR
se_log_or <- sqrt((1 / events_treatment) + (1 / (n_treatment - events_treatment)) + (1 / events_control) + (1 / (n_control - events_control)))
# Define the MCID as an absolute risk reduction of 1.3%
mcid_arr <- 0.013
# Convert the absolute risk reduction to logOR for MCID
# Assuming a baseline risk from the control group
baseline_risk <- events_control / n_control
# Calculate the risk in the treatment group
risk_treatment <- baseline_risk - mcid_arr
# Calculate the odds for control and treatment
odds_control <- baseline_risk / (1 - baseline_risk)
odds_treatment <- risk_treatment / (1 - risk_treatment)
# Calculate the odds ratio (OR)
or <- odds_treatment / odds_control
# For Bayesian analysis, we often use the natural log of the OR
log_or <- log(or)
# Define the priors based on the specifications
# Skeptical Prior: 90% certainty of no clinically relevant effect, so only 10% of area
skeptical_sd <- abs(log_mcid / qnorm(0.10))
skeptical_prior <- list(mu = 0, sigma = skeptical_sd)
# Enthusiastic Prior: 30% probability of harm
enthusiastic_sd <- abs(log_mcid) / qnorm(0.70)
enthusiastic_prior <- list(mu = log_mcid, sigma = enthusiastic_sd)
# Pessimistic Prior: 30% probability of benefit
pessimistic_sd <- abs(log_mcid) / qnorm(0.70)
pessimistic_prior <- list(mu = -log_mcid, sigma = pessimistic_sd)
# Output the calculated values and the priors
list(
log_or = log_or,
se_log_or = se_log_or,
skeptical_prior = skeptical_prior,
enthusiastic_prior = enthusiastic_prior,
pessimistic_prior = pessimistic_prior
)
# Function to calculate the posterior
calc_conjugate_posterior <- function(log_or, se, prior) {
var_prior <- prior$sigma^2
var_data <- se^2
var_post <- 1 / (1 / var_prior + 1 / var_data)
mu_post <- var_post * (log_or / var_data + prior$mu / var_prior)
return(list(mu = mu_post, sigma = sqrt(var_post)))
}
# Calculate the posterior for the skeptical prior
posterior_skeptical <- calc_conjugate_posterior(log_or, se_log_or, skeptical_prior)
posterior_enthus <- calc_conjugate_posterior(log_or, se_log_or, enthusiastic_prior)
posterior_pess <- calc_conjugate_posterior(log_or, se_log_or, pessimistic_prior)
posterior_skeptical
posterior_enthus
posterior_pess
# Calculate the standard error of the logOR
se_log_or <- sqrt((1 / events_treatment) + (1 / (n_treatment - events_treatment)) + (1 / events_control) + (1 / (n_control - events_control)))
# Define the MCID as an absolute risk reduction of 1.3%
mcid_arr <- 0.013
# Convert the absolute risk reduction to logOR for MCID
# Assuming a baseline risk from the control group
baseline_risk <- events_control / n_control
# Calculate the risk in the treatment group
risk_treatment <- baseline_risk - mcid_arr
# Calculate the odds for control and treatment
odds_control <- baseline_risk / (1 - baseline_risk)
odds_treatment <- risk_treatment / (1 - risk_treatment)
# Calculate the odds ratio (OR)
or <- odds_treatment / odds_control
# For Bayesian analysis, we often use the natural log of the OR
log_or <- log(or)
# Define the priors based on the specifications
# Skeptical Prior: 90% certainty of no clinically relevant effect, so only 10% of area
skeptical_sd <- abs(log_mcid / qnorm(0.10))
skeptical_prior <- list(mu = 0, sigma = skeptical_sd)
# Enthusiastic Prior: 30% probability of harm
enthusiastic_sd <- abs(log_mcid) / qnorm(0.70)
enthusiastic_prior <- list(mu = log_mcid, sigma = enthusiastic_sd)
# Pessimistic Prior: 30% probability of benefit
pessimistic_sd <- abs(log_mcid) / qnorm(0.70)
pessimistic_prior <- list(mu = -log_mcid, sigma = pessimistic_sd)
# Output the calculated values and the priors
list(
log_or = log_or,
se_log_or = se_log_or,
skeptical_prior = skeptical_prior,
enthusiastic_prior = enthusiastic_prior,
pessimistic_prior = pessimistic_prior
)
stan_model_code <- "
data {
real logOR;             // Log odds ratio
real<lower=0> SE;       // Standard error
real<lower=0> sd_prior; // Standard deviation of the  prior
real mu_prior; // mean of the  prior
}
parameters {
real theta;  // Parameter (log odds ratio) to estimate
}
model {
// Skeptical prior for theta
theta ~ normal(mu_prior, sd_prior);
// Likelihood
logOR ~ normal(theta, SE);
}
"
stan_data <- list(logOR = log_or, SE = se_log_or, sd_prior = 20,mu_prior=0)
# Run MCMC simulation
fit <- stan(model_code = stan_model_code, data = stan_data, iter = 4000, chains = 4,verbose = FALSE,refresh=0)
l=unlist(apply(round_any(fos[grep(pattern = "AGE",names(fos))],accuracy = 5),1,function(x){which(x==65)}))
round_any
source("~/multistate2/code/utils.R")
source("~/multistate2//code/smoothtest.R")
source("~/multistate2//code/newsmooth.R")
source("~/multistate2/code/fitarray.R")
library("reshape2")
source("~/multistate2/code/arrayindicate.R")
round_any
round(56/5)*5
round(56/5,digits = 1)*5
round(56/5,digits = 0)*5
round(56/5,digits = 1)
l=unlist(apply(round_any(fos[grep(pattern = "AGE",names(fos))],accuracy = 5),1,function(x){which(x==65)}))
table(l)
l70=unlist(apply(round_any(fos[grep(pattern = "AGE",names(fos))],accuracy = 5),1,function(x){which(x==65)}))
l70=unlist(apply(round_any(fos[grep(pattern = "AGE",names(fos))],accuracy = 5),1,function(x){which(x==70)}))
l70
l65=unlist(apply(round_any(fos[grep(pattern = "AGE",names(fos))],accuracy = 5),1,function(x){which(x==65)}))
l70=unlist(apply(round_any(fos[grep(pattern = "AGE",names(fos))],accuracy = 5),1,function(x){which(x==70)}))
l65
l65=unlist(apply(round_any(fos[grep(pattern = "AGE",names(fos))],accuracy = 5),1,function(x){which(x==65)}))
table(l65)
l70=unlist(apply(round_any(fos[grep(pattern = "AGE",names(fos))],accuracy = 5),1,function(x){which(x==70)}))
lable(l70)
l65=unlist(apply(round_any(fos[grep(pattern = "AGE",names(fos))],accuracy = 5),1,function(x){which(x==65)}))
table(l65)
l70=unlist(apply(round_any(fos[grep(pattern = "AGE",names(fos))],accuracy = 5),1,function(x){which(x==70)}))
table(l70)
table(round(fos$AGE1/5*5))
table(round(fos$AGE1/5)*5))
table(round(fos$AGE1/5)*5)
table(round(fos$AGE1/5)*5)
able(round(fos$AGE1/5)*5)
table(round(fos$AGE1/5)*5)
## so we see that after age 55 number of folks we have first measurement data on is limited, so maybe we should relax restriction
l65=unlist(apply(round_any(fos[grep(pattern = "AGE",names(fos))],accuracy = 5),1,function(x){which(x==65)}))
table(l65)
l70=unlist(apply(round_any(fos[grep(pattern = "AGE",names(fos))],accuracy = 5),1,function(x){which(x==70)}))
table(l70)
l60=unlist(apply(round_any(fos[grep(pattern = "AGE",names(fos))],accuracy = 5),1,function(x){which(x==60)}))
table(l60)
l65=unlist(apply(round_any(fos[grep(pattern = "AGE",names(fos))],accuracy = 5),1,function(x){which(x==65)}))
table(l65)
l70=unlist(apply(round_any(fos[grep(pattern = "AGE",names(fos))],accuracy = 5),1,function(x){which(x==70)}))
table(l70)
df=fread("~/Library/CloudStorage/Dropbox-Personal/c3po_covars_121123.csv")
df=fread("~/Library/CloudStorage/Dropbox-PartnersHealthCare/MGBB (1)/Individual Folders/Sarah U//c3po_covars_121123.csv")
df=fread("~/Library/CloudStorage/Dropbox-PartnersHealthCare/MGBB (1)/Individual Folders/Sarah U//c3po_covars_121123.csv")
df=fread("~/Library/CloudStorage/Dropbox-PartnersHealthCare/MGBB (1)/Individual Folders/Sarah U//c3po_covars_121123.csv")
df=fread("~/Library/CloudStorage/Dropbox-PartnersHealthCare/MGBB (1)/Individual Folders/Sarah U//c3po_covars_121123.csv")
df=fread("~/Library/CloudStorage/Dropbox-PartnersHealthCare/MGBB (1)/Individual Folders/Sarah U/multistate_c3po/c3po_covars_121123.csv")
head(df)
16032/365
summary(round(df$start_fu/5)*5)
summary(round(df$start_fu/365.25))
sum(round(df$start_fu/365.25)<41)
length(round(df$start_fu/365.25)<41)
sum(round(df$start_fu/365.25)<41)
dim9df
dim(df)
